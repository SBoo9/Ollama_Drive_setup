{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNpxlRxpJWYUX6VwNSQVyN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SBoo9/Ollama_Drive_setup/blob/main/Ollama_drive_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItpkvgSJPwv0",
        "outputId": "886d92ab-3e47-4595-e2dd-2c59f48999a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5cum9n9P0bp",
        "outputId": "aed710f4-caff-46c3-dafa-cdbd9a9377f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (3.18.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (4.3.8)\n",
            "Downloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.9 virtualenv-20.31.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!virtualenv /content/drive/MyDrive/colab_env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgH8ATdKP9SC",
        "outputId": "54fd7b59-9c9b-4997-b5fd-be777685e180"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created virtual environment CPython3.11.12.final.0-64 in 23593ms\n",
            "  creator CPython3Posix(dest=/content/drive/MyDrive/colab_env, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==25.1.1, setuptools==80.3.1\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source /content/drive/MyDrive/colab_env/bin/activate"
      ],
      "metadata": {
        "id": "AbK3D9F5QBsY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Setup directories in Google Drive\n",
        "DRIVE_PATH = \"/content/drive/MyDrive\"\n",
        "OLLAMA_DIR = f\"{DRIVE_PATH}/ollama_setup\"\n",
        "OLLAMA_BIN_DIR = f\"{OLLAMA_DIR}/bin\"\n",
        "OLLAMA_MODELS_DIR = f\"{OLLAMA_DIR}/models\""
      ],
      "metadata": {
        "id": "YjeCitkuQKOe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directories if they don't exist\n",
        "os.makedirs(OLLAMA_BIN_DIR, exist_ok=True)\n",
        "os.makedirs(OLLAMA_MODELS_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "qfN11L7sQONW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Ollama directory: {OLLAMA_DIR}\")\n",
        "print(f\"Models directory: {OLLAMA_MODELS_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpFxKNKHQRxU",
        "outputId": "52002b1f-8d74-44a5-cebd-144927a84dab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama directory: /content/drive/MyDrive/ollama_setup\n",
            "Models directory: /content/drive/MyDrive/ollama_setup/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Download and install Ollama (only if not already present)\n",
        "def install_ollama():\n",
        "    ollama_binary = f\"{OLLAMA_BIN_DIR}/ollama\"\n",
        "\n",
        "    if os.path.exists(ollama_binary):\n",
        "        print(\"Ollama already installed in Google Drive!\")\n",
        "        return ollama_binary\n",
        "\n",
        "    print(\"Installing Ollama...\")\n",
        "\n",
        "    # Download Ollama\n",
        "    subprocess.run([\n",
        "        \"curl\", \"-fsSL\", \"https://ollama.com/install.sh\"\n",
        "    ], capture_output=False)\n",
        "\n",
        "    # Download the binary directly\n",
        "    subprocess.run([\n",
        "        \"curl\", \"-L\", \"https://github.com/ollama/ollama/releases/download/v0.1.48/ollama-linux-amd64\",\n",
        "        \"-o\", ollama_binary\n",
        "    ], check=True)\n",
        "\n",
        "    # Make it executable\n",
        "    os.chmod(ollama_binary, 0o755)\n",
        "\n",
        "    print(\"Ollama installed successfully!\")\n",
        "    return ollama_binary"
      ],
      "metadata": {
        "id": "P7aYpquTQUlY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ollama_path = install_ollama()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFGBq64lQaoj",
        "outputId": "5da14b0e-f1d4-4894-b551-b30ec33e945f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Ollama...\n",
            "Ollama installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Setup environment variables\n",
        "os.environ['OLLAMA_MODELS'] = OLLAMA_MODELS_DIR\n",
        "os.environ['PATH'] = f\"{OLLAMA_BIN_DIR}:{os.environ.get('PATH', '')}\""
      ],
      "metadata": {
        "id": "4OVP8LypQeDC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Start Ollama server\n",
        "def start_ollama_server():\n",
        "    print(\"Starting Ollama server...\")\n",
        "\n",
        "    # Kill any existing ollama processes\n",
        "    subprocess.run([\"pkill\", \"-f\", \"ollama\"], capture_output=True)\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Start ollama serve in background\n",
        "    process = subprocess.Popen([\n",
        "        ollama_path, \"serve\"\n",
        "    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    # Wait for server to start\n",
        "    time.sleep(5)\n",
        "\n",
        "    # Check if server is running\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"✅ Ollama server started successfully!\")\n",
        "            return process\n",
        "        else:\n",
        "            print(\"❌ Server not responding properly\")\n",
        "            return None\n",
        "    except requests.exceptions.RequestException:\n",
        "        print(\"❌ Failed to connect to Ollama server\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "IiiBk0ZRRFKW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "server_process = start_ollama_server()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYNYR_i3SB0K",
        "outputId": "bdac3dff-950f-4515-a34f-0424d607fb19"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Ollama server...\n",
            "✅ Ollama server started successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_available_models():\n",
        "    \"\"\"List models available for download\"\"\"\n",
        "    popular_models = [\n",
        "        \"llama3.2:1b\",      # Small, fast model\n",
        "        \"llama3.2:3b\",      # Medium model\n",
        "        \"gemma2:2b\",        # Google's Gemma\n",
        "        \"phi3:mini\",        # Microsoft's Phi-3\n",
        "        \"mistral:7b\",       # Mistral 7B\n",
        "        \"codellama:7b\",     # Code-focused model\n",
        "    ]\n",
        "\n",
        "    print(\"Popular models you can download:\")\n",
        "    for model in popular_models:\n",
        "        print(f\"  - {model}\")\n",
        "\n",
        "    return popular_models"
      ],
      "metadata": {
        "id": "y0E-ns9xSEcJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_model(model_name):\n",
        "    \"\"\"Download a model to persistent storage\"\"\"\n",
        "    print(f\"Downloading {model_name}...\")\n",
        "\n",
        "    result = subprocess.run([\n",
        "        ollama_path, \"pull\", model_name\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(f\"✅ {model_name} downloaded successfully!\")\n",
        "        print(f\"Model stored in: {OLLAMA_MODELS_DIR}\")\n",
        "    else:\n",
        "        print(f\"❌ Failed to download {model_name}\")\n",
        "        print(f\"Error: {result.stderr}\")"
      ],
      "metadata": {
        "id": "FqBq7ik4SVzN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_installed_models():\n",
        "    \"\"\"List models already downloaded\"\"\"\n",
        "    result = subprocess.run([\n",
        "        ollama_path, \"list\"\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(\"Installed models:\")\n",
        "        print(result.stdout)\n",
        "    else:\n",
        "        print(\"No models installed yet\")"
      ],
      "metadata": {
        "id": "fqnWj1zpSYLC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_model(model_name, prompt):\n",
        "    \"\"\"Chat with a model\"\"\"\n",
        "    result = subprocess.run([\n",
        "        ollama_path, \"run\", model_name, prompt\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        return result.stdout.strip()\n",
        "    else:\n",
        "        return f\"Error: {result.stderr}\""
      ],
      "metadata": {
        "id": "MftPBaj0SacQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Test the setup\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"OLLAMA SETUP COMPLETE!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# List available models\n",
        "list_available_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3zWpvNsScj1",
        "outputId": "4b9d9e74-d3ca-4c9e-a886-7b8edcf78e52"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "OLLAMA SETUP COMPLETE!\n",
            "==================================================\n",
            "Popular models you can download:\n",
            "  - llama3.2:1b\n",
            "  - llama3.2:3b\n",
            "  - gemma2:2b\n",
            "  - phi3:mini\n",
            "  - mistral:7b\n",
            "  - codellama:7b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['llama3.2:1b',\n",
              " 'llama3.2:3b',\n",
              " 'gemma2:2b',\n",
              " 'phi3:mini',\n",
              " 'mistral:7b',\n",
              " 'codellama:7b']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nChecking installed models...\")\n",
        "list_installed_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inGDrIAhSfjG",
        "outputId": "7d4460dd-a290-419d-e2e1-805780203687"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking installed models...\n",
            "Installed models:\n",
            "NAME\tID\tSIZE\tMODIFIED \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"USAGE INSTRUCTIONS:\")\n",
        "print(\"=\"*50)\n",
        "print(\"1. Download a model: download_model('llama3.2:1b')\")\n",
        "print(\"2. Chat with model: chat_with_model('llama3.2:1b', 'Hello!')\")\n",
        "print(\"3. List models: list_installed_models()\")\n",
        "print(\"4. Your models are saved in Google Drive and will persist!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waiA2-G6Si0f",
        "outputId": "c437ca9d-4cc1-4196-c47e-2bf82917f060"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "USAGE INSTRUCTIONS:\n",
            "==================================================\n",
            "1. Download a model: download_model('llama3.2:1b')\n",
            "2. Chat with model: chat_with_model('llama3.2:1b', 'Hello!')\n",
            "3. List models: list_installed_models()\n",
            "4. Your models are saved in Google Drive and will persist!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"EXAMPLE USAGE:\")\n",
        "print(\"=\"*30)\n",
        "print(\"\"\"\n",
        "# Download a small model (recommended for testing)\n",
        "download_model('llama3.2:1b')\n",
        "\n",
        "# Chat with the model\n",
        "response = chat_with_model('llama3.2:1b', 'Explain what is machine learning in simple terms')\n",
        "print(response)\n",
        "\n",
        "# For your medical RAG project, you might want:\n",
        "download_model('llama3.2:3b')  # Better performance for complex medical queries\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urNiUVF4SmQg",
        "outputId": "a2cb5092-7357-4771-87b2-82f86cbb525d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "EXAMPLE USAGE:\n",
            "==============================\n",
            "\n",
            "# Download a small model (recommended for testing)\n",
            "download_model('llama3.2:1b')\n",
            "\n",
            "# Chat with the model\n",
            "response = chat_with_model('llama3.2:1b', 'Explain what is machine learning in simple terms')\n",
            "print(response)\n",
            "\n",
            "# For your medical RAG project, you might want:\n",
            "download_model('llama3.2:3b')  # Better performance for complex medical queries\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_rag_environment():\n",
        "    \"\"\"Install additional packages for RAG\"\"\"\n",
        "    packages = [\n",
        "        \"langchain\",\n",
        "        \"chromadb\",\n",
        "        \"sentence-transformers\",\n",
        "        \"transformers\",\n",
        "        \"Pillow\",\n",
        "        \"easyocr\",\n",
        "        \"pypdf\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.run([\"pip\", \"install\", package], capture_output=True)\n",
        "\n",
        "    print(\"✅ RAG environment setup complete!\")\n",
        "\n",
        "# Uncomment the next line to install RAG packages\n",
        "# setup_rag_environment()\n"
      ],
      "metadata": {
        "id": "le_WFmyYStOh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nYour Ollama setup is now persistent in Google Drive!\")\n",
        "print(f\"Location: {OLLAMA_DIR}\")\n",
        "print(\"Next time you open Colab, just run the startup section to reconnect!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMBf8iEpS6h2",
        "outputId": "edb65415-5450-4883-d714-27d01048f10f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Your Ollama setup is now persistent in Google Drive!\n",
            "Location: /content/drive/MyDrive/ollama_setup\n",
            "Next time you open Colab, just run the startup section to reconnect!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "easRhO98S-Cf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}